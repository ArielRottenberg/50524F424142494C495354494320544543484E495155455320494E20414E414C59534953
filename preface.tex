\chapter*{PREFACE}
\markboth{PREFACE}{PREFACE}
\addcontentsline{toc}{chapter}{Preface}

A glance at almost any probability book shows that there has been a large flow of ideas from analysis to probability theory. This book is concerned with the flow of ideas in the opposite direction. The topics covered are those branches of analysis to which probability has contributed something, in new results, new proofs, or new insights.

I had two audiences in mind when I wrote this, the first being students and researchers in probability and the second being students and researchers in analysis. Because of the varied backgrounds one would expect from these two audiences, I have tried to make the book as nearly self-contained as possible. Only the standard material covered in any first-year course in real analysis is needed for the first four chapters. For the fifth chapter, the reader should also be acquainted with what is covered in an undergraduate complex variables course. The basic facts needed from a first-year probability course are presented in the first section of the first chapter.

Chapter \ref{ch1} is a minicourse in stochastic analysis. By the end of Chapter \ref{ch1} the reader will have seen all the probability theory that is needed in the rest of the book. After a preliminary section that gives some of the very basic concepts from probability, Section \ref{ch1_sec2} gives the definition and construction of Brownian motion. This stochastic process provides the basis for the relationship between probability and analysis. There are two main reasons for this: Brownian motion is a Markov process, and Brownian motion is a martingale. The Markov structure is described in Section \ref{ch1_sec3}, including the strong Markov property and a brief account of semigroups. On the martingale side, definitions and basic theorems such as Doob's optional stopping theorem, Doob's inequalities, and the decomposition of supermartingales, are given in Section \ref{ch1_sec4}. One of the interesting aspects of continuous time martingales such as Brownian motion is that one can define a stochastic integral with respect to them, even though their paths are not of bounded variation. These stochastic integrals, and the crucial change of variables formula (It\^o's formula), are constructed in Section \ref{ch1_sec5}. Section \ref{ch1_sec6} is devoted to applications of stochastic integrals. These include change of measure formulas, some very useful inequalities, a brief introduction to stochastic differential equations, and local times. Finally, Section \ref{ch1_sec7} is an account of weak convergence of probability measures.

Chapter \ref{ch2} is on harmonic functions and potential theory. I have tried to cover the major results in sufficient generality for most purposes while aiming the material toward applications in later chapters. The first section describes the basics of harmonic functions, Poisson kernels for the ball and the upper half-space, and the solution of the Dirichlet problem. The second section is a short section giving Choquet's capacitability theorem and its use to prove that hitting times of Borel sets are measurable. In Section \ref{ch2_sec3} Newtonian potentials are described, as is the definition of Green functions of a domain. Section \ref{ch2_sec4} proves the symmetry of the transition densities of Brownian motion killed on leaving an arbitrary set and describes the solution to the heat equation by means of eigenvalue expansions of the transition densities of Brownian motion. Section \ref{ch2_sec5} is devoted to a description of the Newtonian capacity of a set and applications. Section \ref{ch2_sec6} is about excessive functions; included is a proof of the Riesz decomposition of superharmonic functions. The last section, Section \ref{ch2_sec7}, gives a construction of the Martin boundary; a complete proof of Choquet's theorem on representations in terms of extremal elements is also provided.

The material in the first two chapters is mostly classical. What is covered in the last three chapters is mostly results that have been proved since the late 1970s and that for the most part have appeared only in journal articles.

Chapter \ref{ch3} is concerned with the behavior of harmonic functions in Lipschitz domains. This topic is considered fundamental to harmonic analysis, and there is a strong interplay here between probability and analysis. First, a technical tool, the boundary Harnack principle, is proved in two different ways. In Section \ref{ch3_sec2}, it is shown how the boundary Harnack principle implies that the Martin boundary for a Lipschitz domain may be identified with the Euclidean boundary. Section \ref{ch3_sec3} is a description of the conditional lifetime problem and its close relative, the conditional gauge theorem. Fatou theorems for Lipschitz domains are proved in Section \ref{ch3_sec4}; again the boundary Harnack principle is key. Section \ref{ch3_sec5} is a description of the support of harmonic measure in Lipschitz domains; as part of the proofs an account of Gehring's inequality and reverse H\"older inequalities is provided.

The subject of Chapter \ref{ch4} is topics related to singular integrals. I have tried to concentrate primarily on those aspects that have a probability component. Section \ref{ch4_sec1} is about maximal functions, while Section \ref{ch4_sec2} describes the Hilbert transform, Riesz' theorem, and the weak (1-1) inequality. Section \ref{ch4_sec3} describes how to extend the methods used in the Hilbert transform case to higher dimensions and gives applications: Riesz transforms, Sobolev inequalities, and the spaces $C^\alpha$. Much more general singular integral operators are possible; the necessary tools such as the Littlewood-Paley inequalities are provided in Section \ref{ch4_sec4}, while Section \ref{ch4_sec5} gives the applications to singular integral operators and Fourier multiplier operators. The last two sections are concerned with the spaces $H^1$ and $BMO$, respectively: Section \ref{ch4_sec6} is devoted to various characterizations of the space $H^1$, and Section \ref{ch4_sec7} proves the $H^1-BMO$ duality and some sharp inequalities regarding singular integral operators.

The final chapter, Chapter \ref{ch5}, is on analytic functions. The first section proves L\'evy's theorem and gives several applications, including the Phragm\'en-Lindel\"of theorem and the Riemann mapping theorem. Section \ref{ch5_sec2} is concerned with giving probabilistic proofs of two important theorems, Picard's little theorem and the Koebe distortion theorem. Section \ref{ch5_sec3} describes the boundary behavior of analytic functions; included are Privalov's theorem, Plessner's theorem, and Makarov's law of the iterated logarithm. Closely related is Section \ref{ch5_sec4}; there probabilistic proofs of Beurling's projection theorem and the Hall projection theorem are provided, as is Makarov's description of the Hausdorff dimension of the support of harmonic measure. The angular derivative problem and an account of Burdzy's theorem is the subject of Section \ref{ch5_sec5}. Finally, in Section 6 Varopoulos' probabilistic proof of the famous corona problem is described.

In almost every section of the last three chapters are a number of interesting open problems. Many of these are quite hard and would provide good topics of research. More in line with the needs of the beginning student are the exercises that are provided at the end of each chapter. These vary from routine to quite difficult. Also at the end of each chapter are notes that describe where I obtained the material. These are not meant to be complete, and the references mentioned in the notes need to be consulted to get the full story.

Much of the material in this book has been covered, some of it several times, in courses at the University of Washington. My thanks go to the students who patiently helped me work through the details. Special thanks go to several people who read parts of the manuscript and made valuable suggestions: Siva Athreya, Rodrigo Ba\~nuelos, Martin Barlow, Krzysztof Burdzy, Michael Cranston, Ping Gao, Davar Khoshnevisan, Youngmee Kwon, Joseph Leighly, and Carlos Tolmasky. Partial support for this project has been provided by the National Science Foundation grant DMS91-00244.

\newpage

\section*{A few words about notation}

There are a number of different definitions of the Fourier transform. We use
\[
    \widehat{f}(\xi) = \int_{\R^d} e^{\im\xi\cdot x}f(x)\,dx
    % Note: originally, $i$ was used here for the imaginary unit.
\]
in this text. $B(x,r)$ denotes the open ball of radius $r$ centered at $x$. We use $|\cdot|$ to denote either Lebesgue measure in $\R^d$ or, when the meaning is clear, surface measure on the boundary of either a ball or a half-space. We write elements of $\R^d$ as $x = (x^1,\ldots,x^d)$. Although there is a danger of confusion with powers, this makes writing the $i$th component of the point $x_n$ easy. The letter $c$, with or without subscripts, signifies a constant whose value is unimportant and which may change from location to location, even within a line. So the reader should not be alarmed at equations like $cx + 2cx = cx$.